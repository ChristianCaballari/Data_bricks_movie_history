{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a6d7141-df42-45de-8fc2-053a7b4faaf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestion del archivo \"movie.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c09c2d51-43fe-4e58-82b5-d546e5c33fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccfa758f-b8e5-48a0-b978-df4536a19522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"p_environment\",\"\")\n",
    "v_environment = dbutils.widgets.get(\"p_environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a63991b-99b3-4e01-8495-db525c0f90a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"p_file_date\",\"2024-12-30\")\n",
    "v_file_date = dbutils.widgets.get(\"p_file_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd843c9c-f704-47e3-b940-1ca7ee3f8d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../includes/configuration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf392977-611b-476c-990b-759731515a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../includes/commom_functions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21537269-1c67-4f3d-9219-22c91210bdd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 1 - Leer el archivo CSV usando \"DataFrameReader\" de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25603762-694e-4e8c-9db5-ff11068a19d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dddbc12c-4bcb-4238-ac67-bf2aca38389c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movie_schema = StructType( fields= [\n",
    "    StructField(\"movieId\", IntegerType(), False),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"budget\", DoubleType(), True),\n",
    "    StructField(\"homePage\", StringType(), True),\n",
    "    StructField(\"overview\", StringType(), True),\n",
    "    StructField(\"popularity\", DoubleType(), True),\n",
    "    StructField(\"yearReleaseDate\", IntegerType(), True),\n",
    "    StructField(\"releaseDate\", DateType(), True),\n",
    "    StructField(\"revenue\", DoubleType(), True),\n",
    "    StructField(\"durationTime\", IntegerType(), True),\n",
    "    StructField(\"movieStatus\", StringType(), True),\n",
    "    StructField(\"tagline\", StringType(), True),\n",
    "    StructField(\"voteAverage\", DoubleType(), True),\n",
    "    StructField(\"voteCount\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e35bee5-dc86-4840-b78d-4f96f08210db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#option(\"header\", True) => es para que el Data Frame agregue la primera fila como el header\n",
    "movie_df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(movie_schema) \\\n",
    "    .csv(f\"{bronze_folder_path}/{v_file_date}/movie.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "60b3c441-3b3b-4da3-995d-2c28808cbe38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movie_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8392cac-24ba-46e3-98be-755755e852e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 2 - Seleccionar sólo columnas \"requeridas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e0ae862-7bda-408e-9d9f-649403727cfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bb7c2f-1474-4351-8206-ecb7491efdeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movies_selected_df = movie_df.select(col(\"movieId\"), col(\"title\"), col(\"budget\"), col(\"popularity\"), col(\"yearReleaseDate\"), col(\"releaseDate\"), col(\"revenue\"), col(\"durationTime\"), col(\"voteAverage\"), col(\"voteCount\").alias(\"vote_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a17075f-ef4f-4a1f-a97d-43ed2ddabf3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 3 - Cambiar el nombre de las columnas según lo \"requerido\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2dfe475-a7d4-4097-b650-df53d7b8c70f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movies_renamed_df = movies_selected_df \\\n",
    "                    .withColumnRenamed(\"movieId\", \"movie_id\") \\\n",
    "                    .withColumnRenamed(\"yearReleaseDate\", \"year_release_date\") \\\n",
    "                    .withColumnRenamed(\"releaseDate\", \"release_date\") \\\n",
    "                    .withColumnRenamed(\"durationTime\", \"duration_time\") \\\n",
    "                    .withColumnRenamed(\"voteAverage\", \"vote_average\") \\\n",
    "                    .withColumnRenamed(\"voteCount\", \"vote_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e82a40-0759-4e0d-b60a-421bd2482b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movies_renamed_df = movies_selected_df \\\n",
    "                    .withColumnsRenamed({\"movieId\": \"movie_id\", \"yearReleaseDate\": \"year_release_date\", \"releaseDate\": \"release_date\", \"durationTime\": \"duration_time\", \"voteAverage\": \"vote_average\", \"voteCount\": \"vote_count\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32690a52-fc8d-4c9a-b08b-44e34c4e7cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso - 4 Agregar la columnas ingestion_date al Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23508bb9-a927-4f38-8a24-352678713884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7fb029e-ae83-4cc2-ab72-566722e7c681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "movies_final_df = add_ingestion_date(movies_renamed_df) \\\n",
    "                  .withColumn(\"environment\", lit(v_environment)) \\\n",
    "                  .withColumn(\"file_date\", lit(v_file_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1664e935-d59e-473e-9d80-62a9c5e4a4a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 5 - Escribir datos en el Data Lake en formato Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee4089c7-3aa7-431f-a365-8376ececebc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Esto no es optimo, ya que en una particion podriamos tener miles de datos, por lo que no es optimo estar borrarando y creando la particion cada vez que se ejecute el Notebook\n",
    "#overwrite_partition(movies_final_df,\"movie_silver\",\"movies\",\"file_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a183ef-664d-4c29-9991-851080c953e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# movies_final_df.write.mode(\"overwrite\").partitionBy(\"year_release_date\").parquet(f\"{silver_folder_path}/movies\")\n",
    "# movies_final_df.write.mode(\"overwrite\").parquet(f\"{silver_folder_path}/movies\")\n",
    "\n",
    "# Crear una tabla en base al data frame movies_final_df\n",
    "# Creara la tabla movies en a DB que se llame movie_silver\n",
    "# Modulo: Spark SQL - Database / Table / View: seccion: Crear Tabla - Capa Silver\n",
    "\n",
    "# Con overwrite, reemplazamos todos los archivos cada vez que se ejecute este Notebook (Carga completa de datos)\n",
    "#movies_final_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"movie_silver.movies\")\n",
    "\n",
    "# Para este caso vamos hacer una carga incremental de datos, para esto usamos append\n",
    "\n",
    "#Nota: Esto es una tabla administrada por Databricks, esto quiere decir que si la tabla se elimina, los registros tambien se van a eliminar\n",
    "# Append, agrega los registros a la tabla existente, carga incremental de datos\n",
    "#movies_final_df.write.mode(\"append\").partitionBy(\"file_date\").format(\"parquet\").saveAsTable(\"movie_silver.movies\")\n",
    "\n",
    "\n",
    "\n",
    "# Modulo: Delta Lake\n",
    "\n",
    "# Cuando se tengan coincidencias en \"tgt.movie_id = src.movie_id\"\" (whenMatchedUpdateAll()). Pues actuliza todos los valores de las columnas. Es decir en la tabla tgt se va a proceder a actualizar todos los registros en base a la tabla de origen (movies_final_df).\n",
    "\n",
    "# Si no hay concidencias (whenNotMatchedInsertAll()), pues se insertan todos los registros en la tabla tgt.\n",
    "# Es decir, se ingresar los registros de movies_final_df a la tabla tgt.\n",
    "merge_condition = \"tgt.movie_id = src.movie_id AND tgt.file_date = src.file_date\" # Buscar por particion\n",
    "merge_delta_lake(movies_final_df,\"movie_silver\",\"movies\",silver_folder_path,merge_condition,\"file_date\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b99a8e39-0dc2-46be-905f-1f5d3e4f783c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "-- Ahora podemos probar la tabla creada en a carpeta silver (Tabla Administrada por Databricks)\n",
    "SELECT file_date, COUNT(1)\n",
    "FROM movie_silver.movies\n",
    "GROUP BY 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "faffedc6-780c-4ddc-b67b-985b65768af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls /mnt/sacjccmoviehistory/silver/movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a4136a4-bc0f-499e-a398-5167539216df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df = spark.read.parquet(f\"{silver_folder_path}/movies\")\n",
    "df = spark.read.format(\"delta\").load(f\"{silver_folder_path}/movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a30f45d5-9aec-42cc-bf2f-d53fdcd9ecb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36004146-9807-4344-811d-bde180d6df2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3073857830472971,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01.Ingestion_file_movie",
   "widgets": {
    "p_environment": {
     "currentValue": "production",
     "nuid": "5558596e-a640-4021-aa33-72d8ef9c1782",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "p_environment",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_environment",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_file_date": {
     "currentValue": "2024-12-16",
     "nuid": "6a9123b8-ad2b-4438-bcd3-489b5826be0b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2024-12-30",
      "label": null,
      "name": "p_file_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2024-12-30",
      "label": null,
      "name": "p_file_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
